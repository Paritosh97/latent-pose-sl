{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["## Step 1: Install Required Python Packages\n","\n","import os\n","\n","# Setting environment variables for temporary directories\n","os.environ['TEMP'] = '/data/azee_env/sgnify_env/tmp'\n","os.environ['TEMPDIR'] = '/data/azee_env/sgnify_env/tmp'\n","os.environ['TMPDIR'] = '/data/azee_env/sgnify_env/tmp'\n","\n","# Install the necessary Python packages.\n","!pip install bpy numpy tqdm\n","!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["## Step 3: Extract Skeleton Data from a BVH File\n","\n","# Here, we'll run the `extract_skeleton.py` script on one of the BVH files to generate the `skeleton.pt`.\n","import bpy\n","import numpy as np\n","import pickle\n","import mathutils\n","import math\n","import os\n","\n","# Load one of the BVH files to extract the skeleton.\n","bvh_file = os.path.join(mocap_path, os.listdir(mocap_path)[0])\n","print(bvh_file)\n","bpy.ops.import_anim.bvh(filepath=bvh_file)\n","\n","ob = bpy.context.object\n","\n","if ob.type == 'ARMATURE':\n","    arm = ob.data\n","\n","# Print the armature to verify.\n","print(arm)\n","        \n","bone_mapping = {}\n","\n","num_bones = len(arm.bones)\n","\n","# Bone's name to ID mapping.\n","for id, bone in enumerate(arm.bones):\n","    bone_mapping[bone.name] = id\n","\n","# Bone's child-to-parent ID mapping.\n","kintree_table = []\n","for bone in arm.bones:\n","    this_bone_id = bone_mapping[bone.name]\n","    \n","    parent = bone.parent\n","    parent_id = bone_mapping[parent.name] if parent else -1\n","    kintree_table.append([parent_id, this_bone_id])\n","\n","# Bone's LocalRestTransform.\n","J = []\n","for bone in arm.bones:\n","    if bone.parent:\n","        bonetrans = bone.parent.matrix_local.inverted() @ bone.matrix_local\n","    else:\n","        bonetrans = bone.matrix_local\n","    J.append([bonetrans.row[0].xyzw, bonetrans.row[1].xyzw, bonetrans.row[2].xyzw, bonetrans.row[3].xyzw])\n","\n","body_data = {\n","    \"J\": np.array(J),\n","    \"kintree_table\": np.array(kintree_table).T,\n","    \"name_to_id\": bone_mapping\n","}\n","\n","# Save the skeleton data.\n","os.makedirs(\"./data\", exist_ok=True)\n","with open(\"./data/skeleton.pt\", \"wb\") as f:\n","    pickle.dump(body_data, f)\n","\n","print(\"Skeleton data has been extracted and saved to ./data/skeleton.pt\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["## Step 3: OR Download Skeleton Data\n","import os\n","\n","os.makedirs(\"./data\", exist_ok=True)\n","!wget -O skeleton.pt https://nextcloud.lisn.upsaclay.fr/index.php/s/H27FBnZFCcAmNzC/download/skeleton.pt\n","!mv skeleton.pt ./data/skeleton.pt"]},{"cell_type":"code","execution_count":null,"metadata":{"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["## Step 4: Extract Animation Data from BVH Files\n","\n","# This step processes each BVH file and extracts the animation data using the `bpy_import_bvh_and_convert.py` script.\n","\n","import bpy\n","import numpy as np\n","import pickle\n","import sys\n","import os\n","import tqdm\n","\n","# Function to extract the frame count from the BVH file\n","def extract_frame_count(bvh_file):\n","    with open(bvh_file, 'r') as file:\n","        for line in file:\n","            if \"Frames:\" in line:\n","                return int(line.split()[1])\n","    return 0\n","\n","# Function to process a single BVH file\n","def process_bvh_file(bvh_file, mapping, output_dir):\n","    print(\"Processing:\", bvh_file)\n","    \n","    # Extract the frame count from the BVH file\n","    frame_count = extract_frame_count(bvh_file)\n","    print(f\"Extracted frame count from BVH file: {frame_count}\")\n","\n","    # Import the BVH file\n","    bpy.ops.import_anim.bvh(filepath=bvh_file)\n","\n","    # Access the imported armature object and scene\n","    ob = bpy.context.object\n","    sce = bpy.context.scene\n","\n","    # Ensure the object is an armature\n","    if ob and ob.type == 'ARMATURE':\n","        armature = ob\n","\n","        # Set the scene's end frame to the frame count extracted from the BVH file\n","        sce.frame_end = frame_count\n","        print(f\"Setting scene frame_end to: {frame_count}\")\n","\n","        anim_data = np.zeros((frame_count, len(mapping.keys()), 3, 3))\n","\n","        # Iterate over each frame and extract pose data\n","        for f in range(1, frame_count + 1):\n","            sce.frame_set(f)\n","            \n","            for pbone in armature.pose.bones:\n","                mat_local = np.array(pbone.matrix_basis)[:3, :3]\n","                pbone_id = mapping.get(pbone.name, None)\n","                if pbone_id is not None:\n","                    anim_data[f-1, pbone_id, :, :] = mat_local\n","\n","        print(\"Animation Data Shape:\", anim_data.shape)\n","\n","        # Ensure the 'extracted' directory exists, if not, create it\n","        if not os.path.exists(output_dir):\n","            os.makedirs(output_dir)\n","\n","        # Save the extracted animation data\n","        output_file = os.path.join(output_dir, \"{}.pkl\".format(armature.name))\n","        with open(output_file, \"wb\") as f:\n","            pickle.dump(anim_data, f)\n","    else:\n","        print(\"Cannot find armature\")\n","\n","output_dir = \"./dataset/extracted\"\n","\n","# Load the skeleton mapping\n","with open(\"./data/skeleton.pt\", \"rb\") as f:\n","    mapping = pickle.load(f)[\"name_to_id\"]\n","\n","# Collect all BVH files\n","mocap_files = [os.path.join(mocap_path, file) for file in os.listdir(mocap_path) if file.endswith(\".bvh\")]\n","\n","# Process each BVH file\n","for mocap_file in tqdm.tqdm(mocap_files):\n","    process_bvh_file(mocap_file, mapping, output_dir)\n","\n","print(\"All BVH files have been processed.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Download the BVH files from the provided link.\n","#!wget -O extracted.zip https://nextcloud.lisn.upsaclay.fr/index.php/s/7cZsL6oXJ6NekDM/download\n","!wget -O extracted.zip https://nextcloud.lisn.upsaclay.fr/index.php/s/qGXAEcjCWYS57rG/download\n","!unzip extracted.zip -d ./dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-08-15T09:57:27.448259Z","iopub.status.busy":"2024-08-15T09:57:27.447866Z","iopub.status.idle":"2024-08-15T09:57:32.112554Z","shell.execute_reply":"2024-08-15T09:57:32.111437Z","shell.execute_reply.started":"2024-08-15T09:57:27.448224Z"},"trusted":true},"outputs":[],"source":["## Step 5: Prepare Data for VPoser Training\n","\n","# Clone the repository if not already cloned\n","repo_url = \"https://github.com/Paritosh97/latent-pose-sl.git\"\n","repo_dir = \"./latent_pose_sl\"\n","if not os.path.exists(repo_dir):\n","    !git clone {repo_url} {repo_dir}\n","\n","module_path = os.path.abspath(os.path.join('nnutils'))\n","\n","import os\n","import pickle\n","import numpy as np\n","import torch\n","from tqdm import tqdm\n","from latent_pose_sl.nnutils.vposer import VPoser  # Adjust the import if the path changes\n","\n","# Path to the extracted animation data\n","extracted_anim_path = \"./dataset/extracted/\"\n","if not os.path.exists(extracted_anim_path):\n","    raise FileNotFoundError(f\"The directory {extracted_anim_path} does not exist.\")\n","\n","# List all the .pkl files in the directory\n","extracted_anim_files = [\n","    os.path.join(extracted_anim_path, file) \n","    for file in os.listdir(extracted_anim_path) \n","    if file.endswith(\".pkl\")\n","]\n","\n","# Check if there are any files to process\n","if not extracted_anim_files:\n","    raise FileNotFoundError(\"No .pkl files found in the extracted animation directory.\")\n","\n","# Process the animation data\n","poses = []\n","for anim_file in tqdm(extracted_anim_files, desc=\"Processing animation files\"):\n","    with open(anim_file, \"rb\") as f:\n","        anim_data = pickle.load(f)\n","        anim_data = torch.tensor(anim_data, dtype=torch.float32)\n","        anim_data = anim_data.reshape((anim_data.shape[0], 1, anim_data.shape[1], 9))\n","        aa = VPoser.matrot2aa(anim_data)  # Ensure this is the correct method and usage\n","        poses.append(aa)\n","poses = torch.cat(poses, dim=0).squeeze(dim=1).numpy()\n","\n","# Shuffle and split the data\n","np.random.shuffle(poses)\n","lens = len(poses)\n","pose_train, pose_val, pose_test = (\n","    poses[:int(0.7 * lens)],\n","    poses[int(0.7 * lens):int(0.9 * lens)],\n","    poses[int(0.9 * lens):],\n",")\n","\n","# Save the data\n","save_dir = \"./data/train\"\n","os.makedirs(save_dir, exist_ok=True)\n","torch.save(torch.from_numpy(pose_train), os.path.join(save_dir, \"pose_train.pt\"))\n","torch.save(torch.from_numpy(pose_val), os.path.join(save_dir, \"pose_val.pt\"))\n","torch.save(torch.from_numpy(pose_test), os.path.join(save_dir, \"pose_test.pt\"))\n","\n","print(\"Training data has been prepared and saved.\")\n","\n","%cd latent_pose_sl"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-08-15T09:57:34.287786Z","iopub.status.busy":"2024-08-15T09:57:34.287374Z"},"trusted":true},"outputs":[],"source":["import sys\n","\n","# Define the custom paths for the dataset and skeleton file\n","custom_datapath = \"../data/train/pose\"  # Replace with your actual dataset path\n","custom_skeletonpath = \"../data/skeleton.pt\"  # Replace with your actual skeleton file path\n","num_epochs = 300  # Specify the number of epochs if you want to change the default\n","\n","\n","module_path = os.path.abspath(os.path.join('nnutils'))\n","\n","sys.path.insert(0, module_path)\n","\n","import torch\n","\n","# Check if a GPU is available\n","if torch.cuda.is_available():\n","    print(\"GPU is available.\")\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    print(f\"Using device: {device}\")\n","    # Print the name of the GPU\n","    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n","else:\n","    print(\"GPU is not available. Using CPU.\")\n","    \n","!mkdir snapshots\n","\n","# Run the training script with custom paths\n","!python nnutils/train_vposer.py --datapath $custom_datapath --skeletonpath $custom_skeletonpath --epochs $num_epochs"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from IPython.display import FileLink\n","\n","# Path to the best model file\n","model_path = \"./snapshots/E250.pt\"\n","\n","# Create a link to download the file\n","FileLink(model_path)"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30747,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":4}
